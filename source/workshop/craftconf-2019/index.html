<!DOCTYPE html>
<html>
<head>
    <title>Building Data Pipelines with Apache Kafka</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <style type="text/css">
        @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
        @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
        @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

        body { font-family: 'Droid Serif'; }
        h1, h2, h3 {
            font-family: 'Yanone Kaffeesatz';
            font-weight: normal;
        }
        .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }

        /* Custom stuff */
        h1 {
            margin-top: 0;
            margin-bottom: 10px;
        }

        li {
            margin-bottom: 10px;
        }

        /* Nested lists */
        li ul {
            margin-top: 8px;
        }

        .left-column {
            width: 50%;
            float: left;
        }

        .right-column {
            width: 50%;
            float: right;
        }

        .column-30 {
            width: 30%;
        }

        .column-45 {
            width: 45%;
        }

        .column-70 {
            width: 70%;
        }

        .column-20 {
            width: 20%;
        }

        .column-80 {
            width: 80%;
        }

        .column-50 {
            width: 50%;
        }

        .column-33 {
            width: 33%;
        }

        .text-center {
            text-align: center;
        }

        .text-right {
            text-align: right;
        }

        .text-bigger {
            font-size: 150%;
        }

        .text-alot-bigger {
            font-size: 200%;
        }

        .image-bigger img {
            width: 50%;
        }

        h3#data-pipeline  {
            margin-top: 0px;
        }

        #kafka-fundamentals + table {
            width: 100%;
            margin-top: 15%;
        }

        #kafka-fundamentals + table th {
            height: 50px;
            text-align: center;
        }

        #kafka-fundamentals + table td {
            text-align: center;
        }

        #kafka-streams + table {
            width: 100%;
            margin-top: 15%;
        }
    </style>
</head>
<body>
<textarea id="source">

class: center, middle

# Building Data Pipelines with Apache Kafka

## Yaroslav Tkachenko

---

# About me

.left-column.column-30[
![](http://1.gravatar.com/avatar/565c77691f1676e0bab82b4881361f55?size=200)

- Data Pipelines
- Big Data
- Microservices
- Distributed Systems
- DevOps
- ... [and more](http://sap1ens.com/files/resume-yaroslav-tkachenko.pdf)

]

.right-column.column-70[
.text-center[
**Yaroslav (Slava) Tkachenko**, Vancouver, Canada
]

<br />

**Activision Blizzard**, 2+ years

*Senior Software/Data Engineer → Software Architect*

**Mobify**, 6 months

*Senior Software Engineer, Lead*

**Bench Accounting**, 5 years

*Software Engineer → Engineering Lead → Director of Engineering*

**Freelance**, 4 years

*Web Developer*

]

???

- Ask everyone for 10 seconds introductions
- Thank for participation
- Feel free to interrupt me anytime and ask questions

---

# Agenda

- Data Pipelines 101: ingestion, storage, processing
- Kafka fundamentals: topics, partitions, brokers, replication, etc.
- Producer and Consumer APIs
- Kafka Streams as a processing layer
- Kafka Connect for integrating with external systems
- Kafka best practices and tuning

# Requirements

- Basic Java or Scala knowledge
- Docker / Docker Compose for running examples locally

---

# Important links

- Slides: https://sap1ens.com/workshop/craftconf-2019
- GitHub: https://github.com/sap1ens/kafka-workshop

Once you've cloned GitHub repo:
- `docker-compose -f docker-compose.connect.yml pull`
- `docker pull maven:3.5.0-jdk-8`

---

class: center, middle

# Data Pipelines 101

---

# Data Pipelines 101

<br />
<br />
<br />
<br />
<br />
<br />

.text-bigger[
***Data Pipeline*** *is a system capable of ingesting, processing and persisting data in a way that allows data transformation, exploration and analysis in accordance with business requirements.*
]

---

# Data Pipelines 101

Message:
- Metadata: timestamps, context, user, etc.
- Payload: business or operational event (user registered, order shipped, log produced) 

---

# Data Pipelines 101

Data producers:
- Web clickstream
- Application logs
- IoT
- Streaming video/audio
- ...

Data consumers:
- Reporting
- Triggers
- Visualization
- Analytics
- Data Science

---

# Data Pipelines 101

Why not just send data directly from A to B?
- Separation of concerns, less coupling
- Scalability 
- Recovery from failures
- Dynamic and/or flexible routing 
- Sampling, throttling, circuit breaking

---

# Data Pipelines 101 - Log

<br />

.center[
<img src="https://kafka.apache.org/21/images/log_consumer.png" width="600"/>

### Log
]

---

# Data Pipelines 101 - Log

Why log?
- Immutable 
- Maintains ordering
- Many consumers 
- Can go back and forth in history

Very different from a work queue:
- Typically unordered 
- Messages are removed once consumed
- Harder to support multiple consumers

---

# Data Pipelines 101

.center[
![](http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/pipeline.png)
### Data Pipeline
]

---

# Data Pipelines 101 - Ingestion

Traditionally telemetry data is ingested through some sort of **endpoint**, like HTTP API. Main requirements for this endpoint:
- High scalability and reliability
  - Autoscaling
  - Throttling and circuit breaking
  - Minimum amount of 3rd-party calls (sometimes it means no auth!)
- Efficient communication with the Log
  - Persistent TCP connections
  - Batching
  - Compression
  - Retries

As any other API it may have a set of API clients for different platforms / languages.

<br />

Other ways could include:
- Hourly or daily jobs for importing database snapshots
- Bulk uploads from external services

---

# Data Pipelines 101 - Processing

Historically, **batch** and **stream** processing have been used in data pipelines. 

Batch:
- Hourly or daily jobs that consume a big chunk of data and perform ETL (extract-transform-load) operations
- Should have robust scheduling and workflow managment
- Able to efficiently deduplicate and reorder data
- Typically very high latency (hours)

Stream:
- Constantly running processing performing transformations on messages
- Record-at-a-time or microbatch approach
- Should be able to handle always changing traffic patterns
- Typically very low latency (seconds, minutes)

<br />

Modern **lambda** and **kappa** architectres have strong emphasis on stream processing

---

# Data Pipelines 101 - Storage

Typically storage is handled by multiple different components:
- Data Lake: raw data, partitioned and indexed
- Data Warehouse: integrated collection of cleaned, processed and transformed data
- Data Mart: simplified Data Warehouse view on a specific business entity

Due to variety of Big Data frameworks and NoSQL there are multiple dimensions to consider:
- OLTP vs OLAP
- *Hot* vs *Cold* data
- Facts vs Dimensions
- Normalized vs Denormalized 
- Star Schema vs Snowflake Schema

Real world example using AWS (Amazon Web Services):
- S3 as a Data Lake
- Hive on EMR / Redshift as a Data Warehouse (and Data Marts)
- DynamoDb / Elasticache / Elasticsearch as additional caches and data stores

---

# Data Pipelines 101 - Log

.center[
<img src="http://kafka.apache.org/images/apache-kafka.png" width="500"/>
]

\* also Apache Pulsar, Amazon Kinesis, Google Pub/Sub, etc.

---

class: center, middle

# Kafka Fundamentals

---

# Kafka Fundamentals

<br />

.center[
<img src="https://kafka.apache.org/21/images/kafka-apis.png" width="500"/>
]

---

# Kafka Fundamentals

Kafka cluster: 
- Consists from one or many **brokers**
- Requires Zookeeper cluster for *discovery* and *metadata persistence*
- One of the brokers is elected as a *controller*. It's just a role that can be assigned to anyone - Kafka doesn't really have a "master" or "leader" node, so there is no SPOF
- Consumers and producers can use *any* broker to bootstrap. Once they discover full cluster topology they start talking to the *right* broker(s)

???

Draw 3-node ZK cluster and 6-node Kafka cluster

---

# Kafka Fundamentals

<br />
<br />
<br />

.center[
<img src="https://kafka.apache.org/21/images/log_anatomy.png" width="500"/>

Topics are *partitioned* and *replicated* data streams
]

---

# Kafka Fundamentals

| Partition | Leaders | Replicas      | In Sync Replicas |
|-----------|---------|---------------|------------------|
| 0         | 101     | (101,102,103) | (101,102,103)    |
| 1         | 102     | (102,103,104) | (102,103,104)    |
| 2         | 103     | (103,104,101) | (103,104)        |
| 3         | 104     | (104,101,102) | ()               |

---

# Kafka Fundamentals

Let's start Zookeeper and Kafka 🤞

Save this as `docker-compose.yml` and run `docker-compose up`.

```yaml
version: '2.1'
services:
  zookeeper:
    container_name: zookeeper
    image: 'confluentinc/cp-zookeeper:5.1.2'
    ports:
      - 2181:2181
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
  kafka:
    container_name: kafka
    image: 'confluentinc/cp-kafka:5.1.2'
    ports:
      - 9092:9092
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1
      - KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=1
```

---

# Kafka Fundamentals

Connect to the running Kafka container:

```sh
docker exec -it kafka bash
```

Let's create the first topic: 

```sh
kafka-topics --zookeeper zookeeper --create --replication-factor 1 --partitions 10 --topic demo-topic
```

And another one with a single partition (you'll see why):

```sh
kafka-topics --zookeeper zookeeper --create --replication-factor 1 --partitions 1 --topic demo-topic-single-partition
```

List & describe topics:

```sh
kafka-topics --zookeeper zookeeper --list
kafka-topics --zookeeper zookeeper --describe --topic demo-topic
kafka-run-class kafka.tools.GetOffsetShell --broker-list kafka:9092 --topic demo-topic
```

---

class: center, middle

# Kafka Producers & Consumers

---

# Kafka Producers & Consumers

Kafka Message:
- Key
- Value
- Headers

Kafka Producer: 
- Sends a message to a specified topic

Kafka Consumer: 
- Consumes messages from a specified topic (or topics)

---

# Kafka Producers & Consumers

Simple Java producer:

```java
String topic = "demo-topic";
Integer count = 100;

Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("acks", "all");
props.put("retries", 0);
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

try (KafkaProducer<String, String> producer = new KafkaProducer<>(props)) {
    for(Integer i = 0; i < count; i++) {
        ProducerRecord<String, String> data = new ProducerRecord<>(topic, i.toString(), "value-" + i);
        producer.send(data);
    }
}
```

---

# Kafka Producers & Consumers

Simple Scala producer:

```scala
val topic = "demo-topic"
val count = 100

val props = new Properties
props.put("bootstrap.servers", "localhost:9092")
props.put("acks", "all")
props.put("retries", 0)
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer")
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer")

val producer = new KafkaProducer[String, String](props)
try {
  for (i <- 0 to count) {
    val data = new ProducerRecord[String, String](topic, i.toString, "value-" + i)
    producer.send(data)
  }
} finally {
  producer.close()
}
```

---

# Kafka Producers & Consumers

Simple Java consumer:

```java
String topic = "demo-topic";

Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("auto.offset.reset", "earliest");
props.put("group.id", "demo-consumer");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
consumer.subscribe(Collections.singletonList(topic));

while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Long.MAX_VALUE);
    for (ConsumerRecord<String, String> record : records) {
        System.out.println(record.key() + "\t" + record.value());
    }
}
```

---

# Kafka Producers & Consumers

Simple Scala consumer:

```scala
val topic = "demo-topic"

val props = new Properties
props.put("bootstrap.servers", "localhost:9092")
props.put("auto.offset.reset", "earliest")
props.put("group.id", "demo-consumer")
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")

val consumer = new KafkaConsumer[String, String](props)
consumer.subscribe(Collections.singletonList(topic))

while (true) {
  val records = consumer.poll(Long.MaxValue).asScala
  for (record <- records) {
    println(record.key() + "\t" + record.value)
  }
}
```

---

# Kafka Producers & Consumers

Consumer **group id** is very important! 
- Controls how to distribute messages (Point-to-Point vs Publish-Subcribe)
- Used as a unique id when persisting offsets

<br />

.center[
![](https://kafka.apache.org/21/images/consumer-groups.png)
]

---

# Kafka Producers & Consumers

*Producing to Kafka directly* vs *using a proxy*

Direct connection:
- Simple and straightforward
- Low latency
- Can use DNS for *some* maintenance work

Proxy:
- Very flexible
- Can do all kinds of maintenance behind the scenes 
- Some enrichment is possible

---

# Kafka Producers & Consumers

Things to consider when building a reliable producer:
- Retries (handled by Kafka)
- Batching (can be handled by Kafka or the application)
- Throttling (Kafka provides quotas)
- Sampling

---

# Kafka Producers & Consumers

It's also important to understand delivery guarantees and potential workarounds!

At most once:
- May or may not be received
- No duplicates
- Probably missing data

At least once:
- Delivery guaranteed
- Possible duplicates
- No missing data

Exactly once:
- Delivery guaranteed
- No duplicates
- No missing data

---

# Kafka Producers & Consumers

Kafka provides **at least once** delivery by default (via producer retries). Idempotence and transactions were introduced in 0.11, so **exactly once** delivery is also possible (and can be enabled in Kafka Streams with a single config change).

```java
KafkaProducer producer = ...
producer.initTransactions();

KafkaConsumer consumer = ...
consumer.subscribe("inputTopic"));

ConsumerRecords records = consumer.poll(Long.MAX_VALUE);
try {
    producer.beginTransaction();
    for (ConsumerRecord record : records) {
        producer.send(processAndProduceRecord("outputTopic", record));
    }
    producer.sendOffsetsToTransaction(currentOffsets(consumer), groupId);  
    producer.commitTransaction();
} catch (Exception e) {
    producer.abortTransaction();
}
```

---

# Kafka Producers & Consumers

**Message headers** is a very useful concept in messaging and Kafka introduced message header support in 0.11.

They're supported on the protocol level and recognized by most of the clients now.

```java
List<Header> headers = Arrays.asList(
    new RecordHeader("hkey1", "hvalue1".getBytes()),
    new RecordHeader("hkey2", "hvalue2".getBytes())
);

new ProducerRecord<>("topic", 0, "key", "value", headers);
```

---

# Kafka Producers & Consumers - Formats & Schemas

```java
public interface Serde<T> {

    Serializer<T> serializer();

    Deserializer<T> deserializer();
}
```

```java
public interface Serializer<T> {

    byte[] serialize(String topic, T data);

    default byte[] serialize(String topic, Headers headers, T data) {
        return serialize(topic, data);
    }
}
```

```java
public interface Deserializer<T> {

    T deserialize(String topic, byte[] data);

    default T deserialize(String topic, Headers headers, byte[] data) {
        return deserialize(topic, data);
    }
}
```

---

# Kafka Producers & Consumers - Formats & Schemas

```java
public class StringSerializer implements Serializer<String> {
    private String encoding = "UTF8";

    @Override
    public byte[] serialize(String topic, String data) {
        try {
            if (data == null)
                return null;
            else
                return data.getBytes(encoding);
        } catch (UnsupportedEncodingException e) {
            throw new SerializationException("Error when serializing string to byte[] due to unsupported encoding " + encoding);
        }
    }
}
```

Other examples:
- [JsonSerializer](https://github.com/apache/kafka/blob/trunk/connect/json/src/main/java/org/apache/kafka/connect/json/JsonSerializer.java), [JsonDeserializer](https://github.com/apache/kafka/blob/trunk/connect/json/src/main/java/org/apache/kafka/connect/json/JsonDeserializer.java)
- [GenericAvroSerde](https://github.com/confluentinc/schema-registry/blob/master/avro-serde/src/main/java/io/confluent/kafka/streams/serdes/avro/GenericAvroSerde.java)
- [KafkaProtobufSerde](https://github.com/daniel-shuy/kafka-protobuf-serde/blob/master/kafka-protobuf-serde/src/main/java/com/github/daniel/shuy/kafka/protobuf/serde/KafkaProtobufSerde.java)

---

# Kafka Producers & Consumers - Formats & Schemas

We often have some sort of metadata that should be passed together with a message.

Message headers for metadata:
- Very simple standard interface 
- Consumers don't need to fully deserialize the payload, which can be very helpful for filtering/routing
- Very weak support in integration/archival solutions

Message envelope for metadata:
- Requires custom format (JSON, Protobuf, Avro, etc.)
- Consumers must deserialize the payload in order to get metadata
- Metadata will always be passed together with the payload

Ideal solution: **combination of both**

Also possible: custom text or binary protocol / wire format, [example](https://docs.confluent.io/current/schema-registry/serializer-formatter.html#wire-format):
- Very efficient
- Consumers don't need to fully deserialize the payload
- Requires custom format

---

# Kafka Producers & Consumers - Formats & Schemas

**Schema Registry** is a data store and an API to upload and fetch message schemas. Most of the common formats like Avro or Protocol Buffers require schemas for serializing and deserializing messages.

API could look like this:
- POST /schema/**$guid** $payload
- GET /schema/**$guid**

Good Schema Registry supports **immutability**, **versioning** and **validation** for schemas.

[Confluent Schema Registry](https://github.com/confluentinc/schema-registry) is a popular open-source Schema Registry from Confluent.

**Guid** is typically transported as metadata (as a header, envelope field or custom format field) with every message. 

---

# Kafka Producers & Consumers - Formats & Schemas

When modifying schemas, as long as a new **guid** is generated for a new schema, Kafka consumers will be able to deserialize it. 

Things might get complicated downstream from that, for example when writing records to an external data store. Most of the data formats have a way to deal with **Schema Evolution**:
- Avro supports different reader and writer schemas
- Protocol buffers use tag numbers to keep schemas compatible


---

class: center, middle

# Kafka Streams

---

# Kafka Streams

Stream processing:
- Constantly running processing performing transformations on messages/records
- Record-at-a-time or microbatch approach
- Should be able to handle always changing traffic patterns
- Typically very low latency (seconds, minutes)

Main advantages: 
- Low latency gives business agility 
- Stateful processing allows windowing, joining and aggregations
- Can use Kafka as input AND output

Challenges: 
- Requires different mindset
- Operational complexity (reliability and scalability)
- Hard to recalculate / reprocess historical data (for example 1+ year)

---

# Kafka Streams

Why **Kafka Streams**? Other popular stream processing frameworks include **Apache Spark** and **Apache Flink**, however Kafka Streams has one very significant distinction: 

<br />

.center[
_Kafka Streams is just a library_
]

<br />

It doesn't need a special scheduler like YARN, it doesn't rely on any Hadoop dependencies, there is no "cluster". It simply uses Kafka as a data store and it leverages Kafka Consumer protocol internally. It can be added to any existing JVM application.

However, most of the concepts and operations described next would generally apply to any stream processing framework, like Apache Flink or Apache Spark.

---

# Kafka Streams - Concepts

It's possible to run one or many **applications**.

Each application can have multiple **instances**.

Each instance runs a JVM process that uses Kafka Streams *library*. 

Each instance defines a **processor topology**:
- can be defined with *Streams DSL* or low-level *Processor API*
- can be stateless or stateful

---

# Kafka Streams - Concepts

```java
Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-app-v1");
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, "org.apache.kafka.common.serialization.Serdes$StringSerde");
props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, "org.apache.kafka.common.serialization.Serdes$StringSerde");
props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, "exactly_once");

StreamsBuilder builder = new StreamsBuilder();

// topology

KafkaStreams streams = new KafkaStreams(builder.build(), props);
streams.start();
```

---

# Kafka Streams - Concepts

```java
KStream<String, String> stream = builder.stream("input-topic");

KStream<String, Integer> transformed = stream.map(
    (key, value) -> KeyValue.pair(value.toLowerCase(), value.length()));

stream.to("output-topic", Produced.with(Serdes.String(), Serdes.Integer());
```

```scala
import org.apache.kafka.streams.scala.Serdes._
import org.apache.kafka.streams.scala.ImplicitConversions._

val stream: KStream[String, String] = builder.stream("input-topic")

val transformed: KStream[String, Integer] = stream.map((_, value) => (value.toLowerCase, value.length))

stream.to("output-topic")
```

Stateless transformations: *map, mapValues, flatMap, flatMapValues, filter, selectKey, foreach, peek*

---

# Kafka Streams - Concepts

The stream-table duality describes the close relationship between **streams** and **tables**:
- Stream as Table: A stream can be considered a _changelog_ of a table, where each data record in the stream captures a state change of the table
- Table as Stream: A table can be considered a _snapshot_, at a point in time, of the latest value for each key in a stream

---

# Kafka Streams - Concepts

.center[
<img src="http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/stream-table-duality.png" width="600"/>
]

---

# Kafka Streams - Concepts

Simple counter implementation, let's look at an example:

```java
KStream<String, String> stream = builder.stream("input-topic");

KTable<String, Long> aggregatedTable = stream
    .groupByKey()
    .count();

aggregatedTable
    .toStream()
    .to("output-topic");
```

Other aggregations: *aggregate, reduce*

---

# Kafka Streams - Concepts

Time in stream processing:
- Event-time
- Processing-time
- Ingestion-time

Timestamp extractor instructs Kafka Streams what timestamp to use. Examples of built-in extractors: _FailOnInvalidTimestamp_, _WallclockTimestampExtractor_.

It's pretty common to implement a custom extractor:

```java
public interface TimestampExtractor {

    long extract(ConsumerRecord<Object, Object> record, long previousTimestamp);
}
```

---

# Kafka Streams - Concepts

Windowing:
- **Hopping**: fixed-size, overlapping windows
- **Tumbling**: fixed-size, non-overlapping, gap-less windows
- **Sliding**: fixed-size, overlapping windows that work on differences between record timestamps
- **Session**: dynamically-sized, non-overlapping, data-driven windows

Hopping & tumbling windows are very similar:
```java
.windowedBy(TimeWindows.of(Duration.ofMinutes(5).advanceBy(Duration.ofMinutes(1))))
.windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
```

Sliding windows are only applicable to joins:
```java
JoinWindows.of(Duration.ofMinutes(5))
```

Session windows allow creating per-key sessions:
```java
.windowedBy(SessionWindows.with(Duration.ofMinutes(5)))
```

---

# Kafka Streams - Concepts

Now counter with windows:

```java
KStream<String, String> stream = builder.stream("input-topic");

KTable<String, Long> aggregatedTable = stream
    .groupByKey()
    .windowedBy(TimeWindows.of(Duration.ofSeconds(15)))
    .count();

aggregatedTable
    .toStream()
    .to("output-topic");
```

---

# Kafka Streams - Concepts

How to handle out of order messages? Kafka Streams indtroduces grace window period:

```java
.windowedBy(TimeWindows.of(Duration.ofHours(1)).grace(Duration.ofMinutes(10)))
```

More on this topic:
- [Streaming 101: The world beyond batch](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101)
- [Streaming 102: The world beyond batch](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102)

> *A **watermark** is a notion of input completeness with respect to event times. A watermark with a value of time X makes the statement: “all input data with event times less than X have been observed.” As such, watermarks act as a metric of progress when observing an unbounded data source with no known end.*

---

# Kafka Streams - Concepts

Finally, windowed counter with suppression of intermediate updates:

```java
KStream<String, String> stream = builder.stream("input-topic");

KTable<String, Long> aggregatedTable = stream
    .groupByKey()
    .windowedBy(TimeWindows.of(Duration.ofSeconds(15)).grace(Duration.ofSeconds(5)))
    .count()
    .suppress(Suppressed.untilWindowCloses(unbounded()));

aggregatedTable
    .toStream()
    .to("output-topic");
```

---

# Kafka Streams - Concepts

| Join operands           | Type         | (INNER) JOIN  | LEFT JOIN     | OUTER JOIN    |
|-------------------------|--------------|---------------|---------------|---------------|
| KStream-to-KStream      | Windowed     | Supported     | Supported     | Supported     |
| KTable-to-KTable        | Non-windowed | Supported     | Supported     | Supported     |
| KStream-to-KTable       | Non-windowed | Supported     | Supported     | Not Supported |
| KStream-to-GlobalKTable | Non-windowed | Supported     | Supported     | Not Supported |
| KTable-to-GlobalKTable  | N/A          | Not Supported | Not Supported | Not Supported |

---

# Kafka Streams - Concepts

Joining mechanics:
- Joins are performed on message keys. It's possible to select a different message key by using one of the following methods: *selectKey* (obviously), *map*, *flatMap*
- All input topics must be **co-partitioned** (have the same number of partitions and use the same partitioning strategy). This doesn't apply to joins with `GlobalKTable`
- Input messages with null keys are typically ignored

```java
KStream<String, Long> left = ...;
KTable<String, Double> right = ...;

KStream<String, String> joined = left.leftJoin(right,
    (leftValue, rightValue) -> "left=" + leftValue + ", right=" + rightValue,
    Joined.keySerde(Serdes.String()).withValueSerde(Serdes.String())
);
```

---

# Kafka Streams - Concepts

- **At-least-once** semantics, enabled by default (`processing.guarantee="at_least_once"`). Rely on basic Kafka Producer & Consumer functionality 
- **Exactly-once** semantics, has to be enabled explicitly (`processing.guarantee="exactly_once"`). Rely on Kafka transactions and idempotent producers (covered before)

---

# Kafka Streams - Concepts

Kafka Streams provides low-level API for defining:
- **Transformers**, via either `Transformer` or `ValueTransformer` interface
- **Processor**, via `Processor` interface
- They have access to `ProcessorContext`, which contains low-level Kafka metadata like partition and offset
- When implementing stateful Transformers or Processors, **State Store** is used for persisting required state:
    - Can store data in memory or built-in RocksDB database
    - Can be windowed
    - Backed by Kafka changelog

---

# Kafka Streams - Concepts

```java
public class PageEmailNotificationAlert implements Processor<PageId, PageMetadata> {

  private ProcessorContext context;

  @Override
  public void init(ProcessorContext context) {
    this.context = context;

    // any additional initialization
  }

  @Override
  void process(PageId pageId, PageMetadata metadata) {
    // formatting and sending emails
  }

  @Override
  void close() {
    // cleanup if required
  }
}
```

Using in DSL:
```java
.process(() -> new PageEmailNotificationAlert());
```

---

# Kafka Streams - Concepts

Kafka Streams may create internal topics, typically following `<ApplicationId>-<operator name>-<suffix>` convention.

- **Repartition** topics are created every time a message key is changed and required for further stateful processing (for example for aggregation or joining)
- **Changelog** topics are created for backing state stores (for example for aggregation, windowing and or when using a custom state store in Processor/Transformer)

**These topics could add significant overhead to Kafka Cluster!** It's important to monitor and tweak them if needed. Sometimes rewriting complicated DSL topology with a lot of stateful operators with a simple Processor or Transformer can reduce that overhead significantly. 

---

# Kafka Streams - Concepts

.center[
<img src="http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/streams-internal.png" width="600"/>
]

---

# Kafka Streams - Concepts

Kafka Streams uses the concepts of **stream partitions** and **stream tasks** as logical units of its parallelism model. There are close links between Kafka Streams and Kafka in the context of parallelism:

- Each **stream partition** is a totally ordered sequence of data records and maps to a Kafka topic partition.
- A data record in the stream maps to a Kafka message from that topic.
- The keys of data records determine the partitioning of data in both Kafka and Kafka Streams, i.e., how data is routed to specific partitions within topics.

So, the maximum parallelism at which your application may run is bounded by the maximum number of stream tasks, which itself is determined by **maximum** number of partitions of the input topic(s) the application is reading from. 

---

# Kafka Streams - Concepts

.center[
<img src="http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/stream-tasks-1.png" width="400"/>
]

---

# Kafka Streams - Concepts

.center[
<img src="http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/stream-tasks-2.png" width="550"/>
]

---

# Kafka Streams - Example

Now let's combine what we learned together:

```java
KTable<String, User> users = builder.table("users");

KStream<String, UserActivity> usersActivity = builder.stream("user_activity");

KTable<Windowed<String>, Long> activityByConsole = usersActivity
    .leftJoin(users, (activity, user) -> {
        if (user != null) {
            Map<String, String> headers = activity.getMetadata();
            headers.put("console", user.getConsole().toString());
        }
        return activity;
    })
    .map((userId, activity) -> new KeyValue<>(activity.getMetadata().get("console"), activity))
    .groupByKey()
    .windowedBy(TimeWindows.of(Duration.ofSeconds(15)))
    .count();

activityByConsole
    .toStream((windowedConsole, count) -> windowedConsole.toString())
    .to("user_activity_by_console", Produced.with(Serdes.String(), Serdes.Long()));
```

---

# Kafka Streams

**Interactive queries** is a powerful technique to access the state of the application (without using any external data stores). 

- Local state is easy to query programmatically:

  ```java
  ReadOnlyKeyValueStore<String, Long> keyValueStore =
      streams.store("CountsKeyValueStore", QueryableStoreTypes.keyValueStore());

  keyValueStore.get("hello"); // also, 'range', 'all', etc.
  ```
- Querying global / remote state over RPC (HTTP):
    - Configure each *instance* of an application with `application.server` (`host:port`)
    - *Implement* the RPC endpoint 
    - Now instances can discover each other's state stores! And forward request/responses appropriately

---

# Kafka Streams - Applications in Data Pipelines

- Data transformation
    - ETL, analytics, ML
- Deduplication
    - Using custom state stores
- Enrichment
    - Using joins
- Running aggregations

---

class: center, middle

# Kafka Connect

---

# Kafka Connect

.center[
<img src="http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/integration-mess.png" width="700"/>
]

---

# Kafka Connect

.center[
![](http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/connect.png)
]

---

# Kafka Connect

- Data integration framework, open-sourced and Kafka-native
- Stop writing custom adhoc integration components between system A and B! Instead, use connectors:
    - **Source connector** for sending data from a data store to Kafka
    - **Sink connector** for sending data from Kafka to a data store
- State is kept in Kafka
- Configuration driven
- REST API is provided
- Tons of connectors are available already
- Flexible, scalable and extendable

---

# Kafka Connect - Concepts

- Workers and tasks
- Connectors
- Converters
- Transforms 

---

# Kafka Connect - Concepts

- Standalone vs Distributed Workers
- Kafka Connect is just a Worker JVM process
    - Can be configured with Property files
    - Custom extensions (your converters, 3rd-party connectors, etc.) should be available in Java Classpath
    - After that they can be referred with a full classname, like `com.blueapron.connect.protobuf.ProtobufConverter`

- Workers create Tasks responsible for actual work
- Tasks are coordinated & rebalanced using a protocol based on consumer groups

---

# Kafka Connect - Concepts

.center[
![](http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/workers.png)
]

---

# Kafka Connect - Concepts

- Kafka Connect uses [internal data format](https://github.com/apache/kafka/tree/trunk/connect/api/src/main/java/org/apache/kafka/connect/data), it supports primitives and complex types like structs, maps and arrays
- All messages coming to and from Kafka must be converted to/from that internal format using a specified *Converter*
- **Why?** In a truly generic system you don't want to implement JSON Elasticsearch connector or Avro S3 connector... If serialization format is generic enough, all connectors can be reused! And it makes possible to apply generic *Transforms*.

.center[
![](http://sap1ens-archive.s3-website-us-east-1.amazonaws.com/workshop-data-pipelines/connect-converters.png)
]

---

# Kafka Connect - Concepts

- *Transforms* allows to use simple, declarative DSL to define lightweight transformations on data. Can be used with source connectors for pre-processing or sink connectors for post-processing
- Built-in transforms: Cast, Drop, ExtractField, ExtractTopic, Flatten, HoistField, InsertField, MaskField, RegexRouter, ReplaceField, SetSchemaMetadata, TimestampConverter, TimestampRouter, ValueToKey
- You can implement your own Transforms! Just compile and put them in Java classpath
- Example:
  ```
    transforms=InsertKey, CastLong
    transforms.InsertKey.type=org.apache.kafka.connect.transforms.ValueToKey
    transforms.InsertKey.fields=id
    transforms.CastLong.type=org.apache.kafka.connect.transforms.Cast$Key
    transforms.CastLong.spec=int64
  ```

---

# Kafka Connect - Example

Let's use Kafka Connect and try to send a text file to and from Kafka. 

Use `docker-compose.connect.yml` to start up everything that's required with `docker-compose -f docker-compose.connect.yml up`

---

# Kafka Connect - Applications in Data Pipelines

- SQL and NoSQL *data capture* with **source** connectors (*Change Data Capture*)
    - Example: MySQL connector capturing and streaming updates for a set of tables
- Delivering processed data to various destinations (Data Marts, BI, application integration) via **sink** connectors
    - Example: using ElasticSearch connector for sending messages to run adhoc queries and search; using InfluxDB connector for sending metrics to build dashboards
- Data *archival* with **sink** connectors
    - Example: capturing and saving data after various transformations in the pipeline to AWS S3 or HDFS

---

class: center, middle

# Running Kafka in Production

---

# Running Kafka in Production

Kafka brokers tuning and optimizations:
- Kafka likes hardware with good IO, so pay attention to good disks (however SSDs are not required), use XFS or EXT4 with some tuning (https://kafka.apache.org/documentation/#generalfs)
- If you're not bottlenecked on disk IO, you're bottlenecked on network IO
- File descriptor limits should be updated
- Consider running Kafka brokers with 16 CPU cores and using 8 `num.io.threads` and 8 `num.network.threads` (defaults are 8 and 3)
- Kafka brokers don't need a lot of heap, but they use pagecache a lot 
- Kafka broker startup scripts have basic JVM optimizations already
- Large messages require special tuning

---

# Running Kafka in Production

Kafka exposes hundreds of metrics over JMX. Some of them should be used for dashboarding and alerting:
- Incoming msg/s, bytes/s, msg/s per topic, bytes/s per topic
- Outgoing bytes/s
- Leader and partition counts
- **Offline and under-replicated partitions**
- **Active controller count**
- ISR expand and shrink rates
- Leader and follower replication byte rates
- **CPU, RAM and JVM metrics**
- **Disk usage, read and write throughput**
- **Network and IO threads idle %**
- **Zookeeper disconnects**

---

# Running Kafka in Production

**End-to-end** testing for Kafka involves sending a series of messages to the input endpoint, making sure they're passed through the whole pipeline and recording them at the end of the pipeline. No duplicates or lost data is expected.

[kafka-monitor](https://github.com/linkedin/kafka-monitor) is one of the potential solutions.

---

# Running Kafka in Production

Kafka has a powerful built-in CLI, however sometimes we want to use UI too (can be helpful when exposing Kafka to other teams with less experience). [kafka-manager](https://github.com/yahoo/kafka-manager) is a de-facto standard:

.center[
<img src="https://raw.githubusercontent.com/yahoo/kafka-manager/master/img/topic-list.png" width="800"/>
]

---

# Running Kafka in Production

Typical Kafka maintenance tasks include:
- **Rolling restarts** for appliying broker configuration changes. Simply restarting brokers one by one by triggering a controlled JVM shutdown. With replication and min-ISR > 2 nothing should be affected
- **Blue-green** deployments could be used for applying bigger changes (for example going from 0.8 to 0.10)
- **Preferred replica leader election** is a mechanism for ensuring that a leader for a given partition should be a "preferred replica" (first in a list of replicas). It can be triggered with CLI
- **Partition reassignment** is required when a broker is completely lost. Simply adding new broker doesn't rebalance existing partitions, it's a manual process that can be triggered with CLI

---

class: center, middle

# Data Pipelines 201

---

# Data Pipelines 201

If we have time we can talk about:

- Kafka topic naming conventions
- Routing (merging and splitting, filtering)
- Getting message schemas right is important!
- Data governance and linage
- Self-serve ("on-demand") streams
- Data reprocessing and deduplication
- Hive streaming ingestion; handling small files problem

---

# Links

- https://kafka.apache.org/documentation/
- https://www.confluent.io/resources/kafka-the-definitive-guide/
- https://docs.confluent.io/current/build-applications.html
- https://github.com/confluentinc/kafka-streams-examples
- https://www.manning.com/books/kafka-streams-in-action
- http://kappa-architecture.com
- https://dataintensive.net
- https://www.enterpriseintegrationpatterns.com
- https://www.manning.com/books/big-data
- http://streamingsystems.net


---

class: center, middle

# We did it! Thank you!

Please answer quick servey: https://goo.gl/forms/xR9CNke7mhMdq0qk2

    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js" type="text/javascript">
</script>
<script type="text/javascript">
    var slideshow = remark.create({
        ratio: '16:9'
    });
</script>
</body>
</html>